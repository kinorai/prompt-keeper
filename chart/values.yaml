---
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s/helm-charts/app-template-3.2.0/charts/other/app-template/values.schema.json

litellm:
  enabled: true
  db:
    deployStandalone: false
  environmentSecrets:
    - prompt-keeper-litellm
  proxy_config:
    model_list:
  # Use this if you want to make requests to `claude-3-haiku-20240307`,`claude-3-opus-20240229`,`claude-2.1` without defining them on the config.yaml
  # Default models
  # Works for ALL Providers and needs the default provider credentials in .env
    - model_name: "*"
      litellm_params:
        model: openai/*
        api_key: os.environ/OPENAI_API_KEY
    - model_name: "*"
      litellm_params:
        model: anthropic/*
        api_key: os.environ/ANTHROPIC_API_KEY
    - model_name: "*"
      litellm_params:
        model: deepseek/*
        api_key: os.environ/DEEPSEEK_API_KEY
    - model_name: "*"
      litellm_params:
        model: gemini/*
        api_key: os.environ/GEMINI_API_KEY
    # - model_name: "*"
    #   litellm_params:
    #     model: archipels-svc/*
    #     api_key: os.environ/ARCHIPELS_API_KEY
    #     api_base: os.environ/ARCHIPELS_API_BASE
    - model_name: fake-model
      litellm_params:
        model: openai/fake-model
        provider: openai
        mock_response: |
          Here's a comprehensive response to help you with your implementation:

          1. First, make sure to check your configuration thoroughly
          2. Validate all input parameters and data types
          3. Test the integration extensively with various test cases
          4. Document any changes made in detail
          5. Set up proper error handling
          6. Implement logging for debugging
          7. Add input validation checks
          8. Create unit tests for core functionality
          9. Set up monitoring and alerts
          10. Plan for scalability considerations
          11. `inline code` should be highlighted

          Here's a sample implementation to get you started:

          ```python
          def validate_configuration(config: Dict[str, Any]) -> bool:
              """Validate the provided configuration"""
              print("Validating configuration parameters...")
              
              required_fields = ['api_key', 'endpoint', 'model_version']
              for field in required_fields:
                  if field not in config:
                      raise ValueError(f"Missing required field: {field}")
              
              return True

          def setup_logging():
              """Configure logging for the application"""
              logging.basicConfig(
                  level=logging.INFO,
                  format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
              )
              return logging.getLogger(__name__)

          def process_request(data: Dict[str, Any]) -> Dict[str, Any]:
              """Process incoming requests with proper error handling"""
              try:
                  # Validate input
                  if not isinstance(data, dict):
                      raise TypeError("Input must be a dictionary")
                      
                  # Process the request
                  result = perform_operation(data)
                  
                  return {"status": "success", "data": result}
              except Exception as e:
                  return {"status": "error", "message": str(e)}
          ```

          For proper monitoring, consider implementing these metrics:

          1. Request latency
          2. Error rates
          3. System resource usage
          4. API endpoint health
          5. Database connection status

          Additional considerations for production deployment:

          1. Set up CI/CD pipelines
          2. Implement automated testing
          3. Configure proper security measures
          4. Plan for disaster recovery
          5. Document API endpoints

          Remember to follow these best practices:

          1. Use semantic versioning
          2. Keep dependencies updated
          3. Implement proper security measures
          4. Follow coding standards
          5. Review code regularly

          Let me know if you need any clarification on these points or would like more specific examples for your use case!
        api_key: my-fake-key
        api_base: https://exampleopenaiendpoint-production.up.railway.app/
opensearch:
  enabled: true
  master:
    masterOnly: false
    replicaCount: 1
    extraEnvVarsSecret: "prompt-keeper-opensearch"
  data:
    replicaCount: 1
  coordinating:
    replicaCount: 1
  ingest:
    replicaCount: 1

config:
  API_HOST: ""
  FRONT_HOST: ""
  DEMO_PROMPT_KEEPER_API_HOST: ""
  DEMO_PROMPT_KEEPER_FRONT_HOST: ""

# defaultPodOptions:
#   automountServiceAccountToken: false
#   securityContext:
#     fsGroup: 1001
#     fsGroupChangePolicy: "OnRootMismatch"
#     runAsGroup: 1001
#     runAsNonRoot: true
#     runAsUser: 1001
#     seccompProfile:
#       type: RuntimeDefault
#     seLinuxOptions: {}

defaultContainerSecurityContext: &defaultContainerSecurityContext
  allowPrivilegeEscalation: false
  capabilities:
    drop:
      - ALL
  privileged: false
  readOnlyRootFilesystem: true

controllers:
  prompt-keeper:
    enabled: true
    type: deployment
    strategy: RollingUpdate
    revisionHistoryLimit: 20
    containers:
      main:
        image:
          repository : "kinorai/prompt-keeper"
          tag: "1"
          pullPolicy: Always
        env:
          - name: API_BASE_URL
            value: "http://{{.Release.Name}}-api.{{.Release.Namespace}}.svc.cluster.local:8000"
        envFrom:
          - secret: api
          - secret: common
          - secret: opensearch
        ports:
          - containerPort: 3000
        resources: {}
        # resources:
        #   requests:
        #     cpu: 10m
        #     memory: 64Mi
        #   limits:
        #     cpu: 200m
        #     memory: 256Mi
        probes:
          liveness:
            enabled: true
            spec:
              httpGet:
                path: /
                port: http
              initialDelaySeconds: 5
              periodSeconds: 5
              timeoutSeconds: 5
          readiness:
            enabled: true
            spec:
              httpGet:
                path: /
                port: http
              initialDelaySeconds: 5
              periodSeconds: 5
              timeoutSeconds: 5
        # Additional security hardening for nginx
        # securityContext: *defaultContainerSecurityContext

service:
  prompt-keeper:
    enabled: true
    controller: prompt-keeper
    type: ClusterIP
    ports:
      http:
        port: 3000

ingress: []

secrets:
  api:
    enabled: true
    stringData:
      OPENSEARCH_URL: "http://{{ .Release.Name }}-opensearch.{{ .Release.Namespace }}.svc.cluster.local:9200"
      OPENSEARCH_INDEX: "prompt-keeper"
      OPENSEARCH_INDEX_SETTINGS: "{}"
      LITELLM_URL: "http://{{ .Release.Name }}-litellm.{{ .Release.Namespace }}.svc.cluster.local:4000"
  # Secrets are used instead of ConfigMaps because pods are automatically reloaded when Secrets change, unlike ConfigMaps
  common:
    enabled: true
    stringData:
      LOG_LEVEL: "debug"
  # LiteLLM
  litellm:
    enabled: true
    stringData:
      OR_APP_NAME: "Prompt Keeper LiteLLM"
      LITELLM_MASTER_KEY: "sk-1234"
      OPENAI_API_KEY: "sk-1234"
      ANTHROPIC_API_KEY: "sk-1234"
      DEEPSEEK_API_KEY: "sk-1234"
      GEMINI_API_KEY: "sk-1234"
      ARCHIPELS_API_KEY: "sk-1234"
      ARCHIPELS_API_BASE: "https://chat.svc.archipels.io/api"
      # OpenSearch
  opensearch:
    enabled: true
    stringData:
      OPENSEARCH_USERNAME: "azkjfaegfeiaufnjoiahj"
      OPENSEARCH_PASSWORD: "adminadminA2!^eaA325153!!*"
